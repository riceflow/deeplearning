{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# クリーニング部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## クリーニング処理\n",
    "def cleaning_reviews(reviews):\n",
    "    dust_keywords = ['<br />', '【', '】', '(', ')','（',  '）','「', '」', '＋', '+','★', '☆', '※','~', '／', '〜', '*', '^', 'm(_ _)m', '(^^)', '(^^)', 'm(._.)m', 'ヽ=ﾟДﾟ=ﾉ']\n",
    "    for key in dust_keywords:\n",
    "        reviews['コメント'] = reviews['コメント'].str.replace(key, '')\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 当メイン関数は都度実行しなくてよい(クリーニング処理をし直したい場合のみ実行)\n",
    "## メイン関数\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "## ファイルの読込み(全件)\n",
    "#shops = pd.read_csv('shops.txt', delimiter='\\t')\n",
    "reviews = pd.read_csv('reviews.txt', delimiter='\\t')\n",
    "\n",
    "## クリーニング\n",
    "reviews = cleaning_reviews(reviews)\n",
    "\n",
    "reviews.to_csv('reviews2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sudachiの初期化\n",
    "import json\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import config\n",
    "\n",
    "def init_sudachi():\n",
    "    with open(config.SETTINGFILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "    sudachi = dictionary.Dictionary(settings).create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C #モードC(一番長い形)に設定\n",
    "    return sudachi, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 単語の分割\n",
    "import json\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import config\n",
    "\n",
    "def words_tokenize(comment, sudachi, mode):\n",
    "    # tokenizeで変換した結果はオブジェクトなので、surfaceメソッドで形態素解析した結果を返却\n",
    "    return [m.surface() for m in sudachi.tokenize(mode, comment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 情報確認用\n",
    "def printFileInfo(reviews):\n",
    "\n",
    "    # 点数単位の件数\n",
    "    print(reviews.query('91 <= 点数 < 101')['点数'].size)\n",
    "    print(reviews.query('81 <= 点数 < 91')['点数'].size)\n",
    "    print(reviews.query('71 <= 点数 < 81')['点数'].size)\n",
    "    print(reviews.query('61 <= 点数 < 71')['点数'].size)\n",
    "    print(reviews.query('0 <= 点数 < 61')['点数'].size)\n",
    "\n",
    "    # 点数単位のコメントの長さ\n",
    "    [print(str(len(m)) + ',' + m) for m in reviews.query('91 <= 点数 < 101')['コメント']]\n",
    "    [print(str(len(m)) + ',' + m) for m in reviews.query('81 <= 点数 < 91')['コメント']]\n",
    "    [print(str(len(m)) + ',' + m) for m in reviews.query('71 <= 点数 < 81')['コメント']]\n",
    "    [print(str(len(m)) + ',' + m) for m in reviews.query('61 <= 点数 < 71')['コメント']]\n",
    "    [print(str(len(m)) + ',' + m) for m in reviews.query('0 <= 点数 < 61')['コメント']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## ラベルの作成\n",
    "def makeLabel(points):\n",
    "    result = pd.DataFrame(points)\n",
    "    for i, point in enumerate(points):\n",
    "        if point >= 90:\n",
    "            result.at[i, '点数'] = 0\n",
    "        elif point >= 80:\n",
    "            result.at[i, '点数'] = 1\n",
    "        elif point >= 70:\n",
    "            result.at[i, '点数'] = 2\n",
    "        elif point >= 60:\n",
    "            result.at[i, '点数'] = 3\n",
    "        else:\n",
    "            result.at[i, '点数'] = 4\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor:{} (108305, 100)\n",
      "Shape of label tensor:{} (108305, 5)\n",
      "Shape of label tensor:{} (75810, 100)\n",
      "Shape of label tensor:{} (21660, 100)\n",
      "Shape of label tensor:{} (10835, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                5248      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 85,413\n",
      "Trainable params: 85,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 75810 samples, validate on 21660 samples\n",
      "Epoch 1/30\n",
      "75810/75810 [==============================] - 74s 974us/step - loss: 1.2035 - acc: 0.4644 - val_loss: 1.1269 - val_acc: 0.5045\n",
      "Epoch 2/30\n",
      "75810/75810 [==============================] - 73s 962us/step - loss: 1.0522 - acc: 0.5345 - val_loss: 1.1003 - val_acc: 0.5168\n",
      "Epoch 3/30\n",
      "75810/75810 [==============================] - 74s 971us/step - loss: 1.0022 - acc: 0.5600 - val_loss: 1.0864 - val_acc: 0.5259\n",
      "Epoch 4/30\n",
      "75810/75810 [==============================] - 74s 981us/step - loss: 0.9717 - acc: 0.5763 - val_loss: 1.0903 - val_acc: 0.5222\n",
      "Epoch 5/30\n",
      "75810/75810 [==============================] - 75s 989us/step - loss: 0.9466 - acc: 0.5870 - val_loss: 1.1074 - val_acc: 0.5246\n",
      "Epoch 6/30\n",
      "75810/75810 [==============================] - 75s 991us/step - loss: 0.9260 - acc: 0.5999 - val_loss: 1.1014 - val_acc: 0.5267\n",
      "Epoch 7/30\n",
      "75810/75810 [==============================] - 78s 1ms/step - loss: 0.9046 - acc: 0.6090 - val_loss: 1.1115 - val_acc: 0.5202\n",
      "Epoch 8/30\n",
      "75810/75810 [==============================] - 77s 1ms/step - loss: 0.8827 - acc: 0.6213 - val_loss: 1.1118 - val_acc: 0.5290\n",
      "Epoch 9/30\n",
      " 8928/75810 [==>...........................] - ETA: 1:03 - loss: 0.8407 - acc: 0.6446"
     ]
    }
   ],
   "source": [
    "## メイン関数\n",
    "import sys\n",
    "import codecs\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# ファイルの読込み(全件)\n",
    "#reviews = pd.read_csv('reviews2.txt', delimiter=',')\n",
    "\n",
    "# ファイルの読込み(50000件)\n",
    "reviews = pd.read_csv('reviews3.txt', delimiter=',')\n",
    "\n",
    "# ファイル情報\n",
    "#printFileInfo(reviews)\n",
    "\n",
    "## Sudachi初期化\n",
    "sudachi, mode = init_sudachi()\n",
    "\n",
    "## 単語分割\n",
    "tokenized_text_list = [words_tokenize(texts, sudachi, mode) for texts in reviews['コメント']]\n",
    "\n",
    "## 形態素解析した結果をシーケンス変換するクラスを生成(See. https://keras.io/ja/preprocessing/text/#tokenizer)\n",
    "max_words = 10000 #一旦全部\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "## シーケンス変換するための事前学習\n",
    "tokenizer.fit_on_texts(tokenized_text_list)\n",
    "\n",
    "## シーケンス変換\n",
    "seq = tokenizer.texts_to_sequences(tokenized_text_list)\n",
    "\n",
    "## 入力値の整形(コメント側)\n",
    "maxlen = 100 #100を超える場合は切り捨て、1000を満たない場合はゼロパティング\n",
    "data = pad_sequences(seq, maxlen=maxlen)\n",
    "\n",
    "## 入力値の整形(点数側)\n",
    "labels = makeLabel(reviews['点数'])\n",
    "categorical_labels = to_categorical(labels)\n",
    "labels = np.asarray(categorical_labels)\n",
    "\n",
    "print(\"Shape of data tensor:{}\", format(data.shape)) # N件, maxlen\n",
    "print(\"Shape of label tensor:{}\", format(labels.shape)) # N件, 点数のカテゴリ化数\n",
    "\n",
    "# 行列をランダムにシャッフル\n",
    "#indices = np.arange(data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "#data = data[indices]\n",
    "#labels = labels[indices]\n",
    "\n",
    "## 評価データと検算データに分割\n",
    "training_samples = data.shape[0] // 100 * 70 # \"//\" で商だけ取得可能\n",
    "validation_samples = data.shape[0] // 100 * 90 # \"//\" で商だけ取得可能\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:validation_samples]\n",
    "y_val = labels[training_samples:validation_samples]\n",
    "x_predict = data[validation_samples:]\n",
    "y_predict = labels[validation_samples:]\n",
    "\n",
    "print(\"Shape of label tensor:{}\", format(x_train.shape)) # N件, 点数のカテゴリ化数\n",
    "print(\"Shape of label tensor:{}\", format(x_val.shape)) # N件, 点数のカテゴリ化数\n",
    "print(\"Shape of label tensor:{}\", format(x_predict.shape)) # N件, 点数のカテゴリ化数\n",
    "\n",
    "## 学習モデルの作成\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 8, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(labels.shape[1], activation='sigmoid')) #点数のカテゴリ化数\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "## 学習開始\n",
    "history = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_split=0.2, validation_data=(x_val, y_val))\n",
    "\n",
    "## モデルの保存\n",
    "# model.sava('pre_trained_model.h5')\n",
    "\n",
    "## 予測\n",
    "results = model.predict(x_predict)\n",
    "prediction_texts = ['90点以上', '80点以上', '70点以上', '60点以上', '60点未満']\n",
    "\n",
    "\n",
    "idx = range(1, x_predict.shape[0] + 1)\n",
    "col = [\"predict\", \"real\"]\n",
    "b = pd.DataFrame(index = idx, columns = col)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "#    print('予測：{0}　答え：{1}'.format(prediction_texts[np.argmax(res)], prediction_texts[np.argmax(y_predict[i])]))\n",
    "    b.iat[i, 0] = prediction_texts[np.argmax(res)]\n",
    "    b.iat[i, 1] = prediction_texts[np.argmax(y_predict[i])]\n",
    "                                   \n",
    "b.to_csv('c:\\\\temp\\\\yone.txt')\n",
    "\n",
    "\n",
    "## 結果のプロット\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Trainig acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Trainig loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# メモ部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predict real\n",
      "1      NaN  NaN\n",
      "2      NaN  NaN\n",
      "3      NaN  NaN\n",
      "4      NaN  NaN\n",
      "5      NaN  NaN\n",
      "6      NaN  NaN\n",
      "7      NaN  NaN\n",
      "8      NaN  NaN\n",
      "9      NaN  NaN\n",
      "10     NaN  NaN\n",
      "   predict real\n",
      "1      NaN  NaN\n",
      "2        1  NaN\n",
      "3      NaN  NaN\n",
      "4      NaN  NaN\n",
      "5      NaN  NaN\n",
      "6      NaN  NaN\n",
      "7      NaN  NaN\n",
      "8      NaN  NaN\n",
      "9      NaN  NaN\n",
      "10     NaN  NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "idx = range(1, 11)\n",
    "col = [\"predict\", \"real\"]\n",
    "a = pd.DataFrame(index = idx, columns = col)\n",
    "\n",
    "print(a)\n",
    "a.iat[1, 0] = 1\n",
    "print(a)\n",
    "\n",
    "#a.to_csv('c:\\\\temp\\\\yone.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Thanks\n",
    "\n",
    "### \"LSTMを使ってテキストの多クラス分類する\".CODING ECHO\n",
    "https://goo.gl/MGJHTB\n",
    "\n",
    "### \"Jupyter Notebook の Tips をまとめてみた\".Tech Blog\n",
    "https://adtech.cyberagent.io/techblog/archives/2317\n",
    "\n",
    "### \"Jupyter 知っておくと少し便利なTIPS集\".Qiita\n",
    "https://qiita.com/simonritchie/items/d7dccb798f0b9c8b1ec5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JupyterNoteBookの使い方忘れそうな部分のメモ\n",
    "\n",
    "### Markdownモード\n",
    "m  \n",
    "その後、記載した後、Shift + Enterでラベルに変換  \n",
    "編集したい場合は、1～6で戻るのが筋がよさげ  \n",
    "rやyでも編集モードに入れるが、Markdownからコード編集モードに戻るので、数値で編集モードに入った方がよさげ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
