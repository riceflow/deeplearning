{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 分類をインデックス化してDataFrameに追加する共通関数(秋山さん作成分)\n",
    "def classify_and_add_indexed_df(df, org_key, new_key):\n",
    "    class_sets = set()\n",
    "    for class_ in df[org_key]:\n",
    "        class_sets.add(class_)\n",
    "\n",
    "    sorted_classes = sorted(list(class_sets))\n",
    "    class_indices = dict((c, i) for i, c in enumerate(sorted_classes)) \n",
    "#    print(class_indices)\n",
    "\n",
    "    classes = []\n",
    "    for i, class_ in enumerate(reviews[org_key]):\n",
    "        classes.append(class_indices[class_])\n",
    "    indexed_df[new_key] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 単語分割(See. http://www.denzow.me/entry/2017/10/29/160903)\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def split_into_words(doc):\n",
    "    \"\"\"\n",
    "    名詞だけを取り出してリストで戻す関数\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t = Tokenizer(mmap=True)\n",
    "        word_list = []\n",
    "        # 形態素して取り出す\n",
    "        for token in t.tokenize(doc):\n",
    "            # 品詞の判定をして、名詞か動詞か形容詞だけを取り出す\n",
    "            if (token.part_of_speech.split(\",\")[0] in (\"名詞\",\"動詞\",\"形容詞\")\n",
    "                and  token.part_of_speech.split(\",\")[1] != \"数\"):  # ただし、数詞は使っても意味が薄いので捨てる\n",
    "                # 表層形を登録する\n",
    "                word_list.append(token)\n",
    "        return word_list\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 辞書作成\n",
    "def make_dict(words, tokens):\n",
    "    for j in range(0, len(tokens)):\n",
    "        token = tokens[j]\n",
    "        is_new_word = True\n",
    "        for i in range(len(words)):\n",
    "            if words[i][0] == token.surface and words[i][1] == token.part_of_speech[:2]:\n",
    "                words[i][2] += 1\n",
    "                is_new_word = False\n",
    "                break\n",
    "        if is_new_word:\n",
    "            words.append([token.surface, token.part_of_speech[:2], 1])\n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 辞書ができたので全セリフデータを固定長の数字列に変換します\n",
    "def trans_words_to_number(words, lines, max_speech_length):\n",
    "  data_list = []\n",
    "  for line in lines:\n",
    "    #tokens = split_into_words(line.replace('<br />', ''))\n",
    "    tokens = Tokenizer().tokenize(line.replace('<br />', ''))\n",
    "    record = []\n",
    "    # 固定長より長いセリフは打ち切り\n",
    "    # 固定長より短いセリフは0埋め\n",
    "    n = min(len(tokens), max_speech_length) #通常はlen(tokens) > max_speech_lengthを想定しているが、実装仮組中の少ないデータ時用\n",
    "    for j in range(0, n):\n",
    "      if (tokens[j].part_of_speech.split(\",\")[0] in (\"名詞\",\"動詞\",\"形容詞\")\n",
    "       and  tokens[j].part_of_speech.split(\",\")[1] != \"数\"):  # ただし、数詞は使っても意味が薄いので捨てる\n",
    "        dic_temp = dic[(dic['words'] == tokens[j].surface) \n",
    "                     & (dic['parts'] == tokens[j].part_of_speech[:2])] #\n",
    "        record.append(dic_temp.index[0] + 1)\n",
    "    if (len(record) < max_speech_length):\n",
    "      for j in range(max_speech_length - len(record)):\n",
    "        record.append(0)\n",
    "    record.append(line) # ログ用に元文字列も付与します（数字列だとどのセリフかわからないので）\n",
    "    data_list.append(record)\n",
    "  return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:102: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(32, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:103: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:105: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(20, dropout=0.1, recurrent_dropout=0.1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "12/12 [==============================] - 10s - loss: 1.6005 - acc: 0.3333          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Accuracy (test) : 33.33%\n",
      "       想定確率                                           答え                    \n",
      "          0         1         2         3         4    0    1    2    3    4\n",
      "0  0.219426  0.190835  0.194573  0.200855  0.194311  0.0  0.0  0.0  1.0  0.0\n",
      "1  0.220980  0.191002  0.194797  0.199529  0.193692  1.0  0.0  0.0  0.0  0.0\n",
      "2  0.216434  0.191229  0.195054  0.201877  0.195405  0.0  0.0  1.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "## メイン関数\n",
    "import sys\n",
    "import codecs\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "## ファイルの読込み(全件)\n",
    "#shops = pd.read_csv('shops.txt', delimiter='\\t')\n",
    "#reviews = pd.read_csv('reviews.txt', delimiter='\\t')\n",
    "\n",
    "## ファイルの読込み(サンプル)\n",
    "shops = pd.read_csv('as.txt', delimiter='\\t')\n",
    "reviews = pd.read_csv('ar.txt', delimiter='\\t')\n",
    "\n",
    "## ファイルの先頭と末端読み(学習データと検証データ分割用)\n",
    "#reviewsh = reviews.head(3)\n",
    "#reviewst = reviews.tail(3)\n",
    "#print(reviewsh)\n",
    "#print(reviewst)\n",
    "\n",
    "##学習用データ前作業\n",
    "### インデックスの作成\n",
    "indexed_df = pd.DataFrame()\n",
    "classify_and_add_indexed_df(reviews, '分類', 'category')\n",
    "classify_and_add_indexed_df(reviews, 'スープ', 'soup')\n",
    "\n",
    "### 文章の形態素解析\n",
    "# 一旦メニュー側は評価しない。\n",
    "#words4menu = [] # 単語文字列、品詞、登場回数のリスト\n",
    "#for i, doc1 in enumerate(reviews['メニュー']):\n",
    "#    words1 = split_into_words(doc1)\n",
    "#    indexed_df.loc[i, 'menu'] = i\n",
    "\n",
    "### 辞書の作成\n",
    "# 各レコードのコメントを分解し、品詞と登場回数をカウント\n",
    "dict4comment = [] # 単語文字列、品詞、登場回数のリスト\n",
    "for i, doc2 in enumerate(reviews['コメント']):\n",
    "    dict4comment = make_dict(dict4comment, split_into_words(doc2.replace('<br />', '')))\n",
    "\n",
    "# 単語情報をデータフレームに変換します\n",
    "dic = pd.DataFrame(dict4comment)\n",
    "dic.columns = ['words', 'parts', 'freq']\n",
    "dic = dic.sort_values(by=['freq'], ascending=False)\n",
    "dic = dic.reset_index(drop=True)\n",
    "num_words = dic.shape[0] # 全単語数を確認しておきます\n",
    "\n",
    "# 辞書ができたので全データを固定長の数字列に変換\n",
    "max_speech_length = 8\n",
    "output_vector = 5\n",
    "data_list = trans_words_to_number(dic, reviews['コメント'], max_speech_length)\n",
    "\n",
    "data_list_piece = []\n",
    "for i, line in enumerate(data_list):\n",
    "  data_list_piece.append(line[0:max_speech_length])\n",
    "\n",
    "ans_list = []\n",
    "for line in enumerate(reviews['点数']):\n",
    "  if line[1] > 80:\n",
    "    ans_list.append(4)\n",
    "  elif line[1] > 60:\n",
    "    ans_list.append(3)\n",
    "  elif line[1] > 40:\n",
    "    ans_list.append(2)\n",
    "  elif line[1] > 20:\n",
    "    ans_list.append(1)\n",
    "  else:\n",
    "    ans_list.append(0)\n",
    "#ans_list = reviews['点数']\n",
    "\n",
    "### 学習データとサンプルデータとの分割\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(data_list_piece, ans_list, train_size=0.8)\n",
    "\n",
    "### 点数データを配列化\n",
    "### See. http://may46onez.hatenablog.com/entry/2016/07/14/122047\n",
    "train_Y = np_utils.to_categorical(train_Y, output_vector)\n",
    "test_Y = np_utils.to_categorical(test_Y, output_vector)\n",
    "\n",
    "#print(np.array(train_X))\n",
    "#print(np.array(train_Y))\n",
    "#print(np.array(test_X))\n",
    "#print(np.array(test_Y))\n",
    "\n",
    "# ランダム関数の初期化\n",
    "random.seed(123)\n",
    "numpy.random.seed(123)\n",
    "\n",
    "## モデルの構築\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_vecor_length, input_length=max_speech_length))\n",
    "model.add(Conv1D(embedding_vecor_length, 3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(20, dropout_W=0.1, dropout_U=0.1))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(output_vector, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#print(model.summary())\n",
    "\n",
    "# モデル訓練\n",
    "model.fit(np.array(train_X), np.array(train_Y), epochs=1, batch_size=1)\n",
    "\n",
    "# モデル評価\n",
    "loss_and_metrics = model.evaluate(np.array(test_X), np.array(test_Y), verbose=0)\n",
    "print(\"Accuracy (test) : %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "prob = pd.DataFrame(model.predict(np.array(test_X)))\n",
    "ans = pd.DataFrame(np.array(test_Y))\n",
    "print(pd.concat([prob, ans], axis=1, keys=['想定確率', '答え']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考URL\n",
    "### LSTMでどのキャラクターのセリフか判別する\n",
    "### https://qiita.com/CookieBox26/items/6823346661f600b246eb\n",
    "### 式を図示化してくれているので分かりやすかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(shops.columns[1])\n",
    "#print(shops['店名'])\n",
    "#print(reviews['コメント'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#validation_dataを追加すると学習率が上がる可能性がある"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
