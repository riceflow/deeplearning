{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# クリーニング部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## クリーニング処理\n",
    "def cleaning_reviews(reviews):\n",
    "    dust_keywords = ['<br />', ' ', '　', '【', '】', '(', ')','（',  '）','「', '」', '＋', '+','★', '☆', '※','~', '／', '〜', '*', '^', 'm(_ _)m', '(^^)', '(^^)', 'm(._.)m', 'ヽ=ﾟДﾟ=ﾉ']\n",
    "    for key in dust_keywords:\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{6}\\n', '', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\+[0-9]{4}\\n', '', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('http(s*)://.+\\n', '。', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('iPhone', '携帯電話')\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('\\n', '。', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].str.replace(key, '')\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 98,
   "metadata": {},
=======
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 7af701d03e20ea35d995af8915e066a82b4e5ad6
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "##### 当メイン関数は都度実行しなくてよい(クリーニング処理をし直したい場合のみ実行)\n",
    "\n",
    "#### 前提条件\n",
    "#・livedoorニュースコーパスをダウンロードして、解凍する。\n",
    "#・textディレクトリ配下のニュースを、当JupyterNoteBookと同じ配下に配置する\n",
    "#(∵GitHubにそのままコミットして公開の位置づけにならないための配慮)\n",
    "\n",
    "## メイン関数\n",
    "import glob\n",
    "import codecs\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "count = 0\n",
    "news_index = 1\n",
    "\n",
    "df = pd.DataFrame([], columns=['CATEGORY', 'NEWS'])\n",
    "count = count + 1\n",
    "dlist1 = glob.glob(\"./text/*\")\n",
    "for dpath1 in dlist1:\n",
    "    dlist2 = glob.glob(dpath1 + \"/*\")\n",
    "    for dpath2 in dlist2:\n",
    "        if 'LICENSE.txt' not in dpath2:\n",
    "            allLines = codecs.open(dpath2, 'r', 'utf-8').read()\n",
    "            df = df.append(pd.DataFrame([[news_index, allLines]], columns=['CATEGORY', 'NEWS'], index=[count]))\n",
    "            count = count + 1\n",
    "    news_index = news_index + 1\n",
    "\n",
    "## ファイルの読込み(全件)\n",
    "#shops = pd.read_csv('shops.txt', delimiter='\\t')\n",
    "#reviews = pd.read_csv('reviews.txt', delimiter='\\t')\n",
    "\n",
    "## クリーニング\n",
    "df2 = cleaning_reviews(df)\n",
    "df2.to_csv('newsmerge.txt')\n",
    "\n",
    "## 確認用\n",
    "#print(df2.NEWS)\n",
    "#print(df2.loc[:, ['NEWS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習部"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 99,
   "metadata": {},
=======
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 7af701d03e20ea35d995af8915e066a82b4e5ad6
   "outputs": [],
   "source": [
    "## Sudachiの初期化\n",
    "import json\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import config\n",
    "\n",
    "def init_sudachi():\n",
    "    with open(config.SETTINGFILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "    sudachi = dictionary.Dictionary(settings).create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C #モードC(一番長い形)に設定\n",
    "    return sudachi, mode"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 100,
   "metadata": {},
=======
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 7af701d03e20ea35d995af8915e066a82b4e5ad6
   "outputs": [],
   "source": [
    "## 単語の分割\n",
    "import json\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import config\n",
    "\n",
    "def words_tokenize(comment, sudachi, mode):\n",
    "    # tokenizeで変換した結果はオブジェクトなので、surfaceメソッドで形態素解析した結果を返却\n",
    "    return [m.surface() for m in sudachi.tokenize(mode, comment)]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 101,
   "metadata": {},
=======
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 7af701d03e20ea35d995af8915e066a82b4e5ad6
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## ラベルの作成\n",
    "def makeLabel(points):\n",
    "    result = pd.DataFrame(points)\n",
    "    for i, point in enumerate(points):\n",
    "        if point == 1:\n",
    "            result.at[i, 'CATEGORY'] = 1\n",
    "        elif point == 2:\n",
    "            result.at[i, 'CATEGORY'] = 2\n",
    "        elif point == 3:\n",
    "            result.at[i, 'CATEGORY'] = 3\n",
    "        elif point == 4:\n",
    "            result.at[i, 'CATEGORY'] = 4\n",
    "        elif point == 5:\n",
    "            result.at[i, 'CATEGORY'] = 5\n",
    "        elif point == 6:\n",
    "            result.at[i, 'CATEGORY'] = 6\n",
    "        elif point == 7:\n",
    "            result.at[i, 'CATEGORY'] = 7\n",
    "        elif point == 8:\n",
    "            result.at[i, 'CATEGORY'] = 8\n",
    "        else:\n",
    "            result.at[i, 'CATEGORY'] = 9\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 102,
=======
   "execution_count": 6,
>>>>>>> 7af701d03e20ea35d995af8915e066a82b4e5ad6
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor:{} (7367, 100)\n",
      "Shape of label tensor:{} (7368, 10)\n",
      "Shape of label tensor:{} (5110, 100)\n",
      "Shape of label tensor:{} (1460, 100)\n",
      "Shape of label tensor:{} (797, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 100, 8)            800000    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 32)                5248      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 805,578\n",
      "Trainable params: 805,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5110 samples, validate on 1460 samples\n",
      "Epoch 1/30\n",
      "5110/5110 [==============================] - 26s 5ms/step - loss: 1.9218 - acc: 0.1699 - val_loss: 6.0423 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 1.5897 - acc: 0.3444 - val_loss: 5.3631 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 1.0894 - acc: 0.5881 - val_loss: 4.0110 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.6529 - acc: 0.7834 - val_loss: 5.9790 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.3801 - acc: 0.8642 - val_loss: 6.0560 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.2664 - acc: 0.8863 - val_loss: 5.9765 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "5110/5110 [==============================] - 24s 5ms/step - loss: 0.1958 - acc: 0.9061 - val_loss: 6.2361 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "5110/5110 [==============================] - 24s 5ms/step - loss: 0.1570 - acc: 0.9145 - val_loss: 6.5284 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.1329 - acc: 0.9241 - val_loss: 6.4292 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.1241 - acc: 0.9245 - val_loss: 6.5880 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0897 - acc: 0.9339 - val_loss: 6.5794 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0812 - acc: 0.9346 - val_loss: 6.9642 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0837 - acc: 0.9327 - val_loss: 7.0867 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0750 - acc: 0.9370 - val_loss: 7.2745 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0743 - acc: 0.9362 - val_loss: 7.4256 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0770 - acc: 0.9372 - val_loss: 7.2870 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0604 - acc: 0.9395 - val_loss: 7.4021 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0733 - acc: 0.9356 - val_loss: 7.8820 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0606 - acc: 0.9387 - val_loss: 7.8276 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0579 - acc: 0.9403 - val_loss: 8.1521 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0588 - acc: 0.9384 - val_loss: 8.4946 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0590 - acc: 0.9389 - val_loss: 8.4171 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0607 - acc: 0.9391 - val_loss: 8.5391 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0487 - acc: 0.9425 - val_loss: 8.6251 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0481 - acc: 0.9423 - val_loss: 8.9414 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0583 - acc: 0.9401 - val_loss: 8.7351 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0525 - acc: 0.9411 - val_loss: 9.0165 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0462 - acc: 0.9425 - val_loss: 9.2039 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0478 - acc: 0.9415 - val_loss: 9.5616 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "5110/5110 [==============================] - 25s 5ms/step - loss: 0.0631 - acc: 0.9389 - val_loss: 9.1414 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-150e9673acf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;31m#    print('予測：{0}　答え：{1}'.format(prediction_texts[np.argmax(res)], prediction_texts[np.argmax(y_predict[i])]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'c:\\\\temp\\\\yone.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-67ecd603a905>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m## シーケンス変換するための事前学習\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m## シーケンス変換\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    186\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                                                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                                                                      self.split)\n\u001b[0m\u001b[0;32m    189\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(text, filters, lower, split)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \"\"\"\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
>>>>>>> 7af701d03e20ea35d995af8915e066a82b4e5ad6
     ]
    }
   ],
   "source": [
    "## メイン関数\n",
    "import sys\n",
    "import codecs\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# ファイルの読込み(全件)\n",
    "#reviews = pd.read_csv('newsmerge.txt', delimiter=',')\n",
    "\n",
    "# データ読込み\n",
    "reviews = df\n",
    "\n",
    "## Sudachi初期化\n",
    "sudachi, mode = init_sudachi()\n",
    "\n",
    "## 単語分割\n",
    "tokenized_text_list = [words_tokenize(texts, sudachi, mode) for texts in reviews['NEWS']]\n",
    "\n",
    "## 形態素解析した結果をシーケンス変換するクラスを生成(See. https://keras.io/ja/preprocessing/text/#tokenizer)\n",
    "max_words = 100000 #一旦全部\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "## シーケンス変換するための事前学習\n",
    "tokenizer.fit_on_texts(tokenized_text_list)\n",
    "\n",
    "## シーケンス変換\n",
    "seq = tokenizer.texts_to_sequences(tokenized_text_list)\n",
    "\n",
    "## 入力値の整形(コメント側)\n",
    "maxlen = 100 #100を超える場合は切り捨て、1000を満たない場合はゼロパティング\n",
    "data = pad_sequences(seq, maxlen=maxlen)\n",
    "\n",
    "## 入力値の整形(点数側)\n",
    "labels = makeLabel(reviews['CATEGORY'])\n",
    "\n",
    "categorical_labels = to_categorical(labels)\n",
    "labels = np.asarray(categorical_labels)\n",
    "\n",
    "print(\"Shape of data tensor:{}\", format(data.shape)) # N件, maxlen\n",
    "print(\"Shape of label tensor:{}\", format(labels.shape)) # N件, 点数のカテゴリ化数\n",
    "\n",
    "# 行列をランダムにシャッフル\n",
    "#indices = np.arange(data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "#data = data[indices]\n",
    "#labels = labels[indices]\n",
    "\n",
    "## 評価データと検算データに分割\n",
    "training_samples = data.shape[0] // 100 * 70 # \"//\" で商だけ取得可能\n",
    "validation_samples = data.shape[0] // 100 * 90 # \"//\" で商だけ取得可能\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:validation_samples]\n",
    "y_val = labels[training_samples:validation_samples]\n",
    "x_predict = data[validation_samples:]\n",
    "y_predict = labels[validation_samples:]\n",
    "\n",
    "print(\"Shape of label tensor:{}\", format(x_train.shape)) # N件, 点数のカテゴリ化数\n",
    "print(\"Shape of label tensor:{}\", format(x_val.shape)) # N件, 点数のカテゴリ化数\n",
    "print(\"Shape of label tensor:{}\", format(x_predict.shape)) # N件, 点数のカテゴリ化数\n",
    "\n",
    "## 学習モデルの作成\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 8, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(labels.shape[1], activation='sigmoid')) #点数のカテゴリ化数\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "## 学習開始\n",
    "history = model.fit(x_train, y_train, epochs=30, batch_size=8, validation_split=0.2, validation_data=(x_val, y_val))\n",
    "\n",
    "## モデルの保存\n",
    "# model.sava('pre_trained_model.h5')\n",
    "\n",
    "## 予測\n",
    "results = model.predict(x_predict)\n",
    "prediction_texts = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "\n",
    "idx = range(1, x_predict.shape[0] + 1)\n",
    "col = [\"predict\", \"real\"]\n",
    "b = pd.DataFrame(index = idx, columns = col)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "#    print('予測：{0}　答え：{1}'.format(prediction_texts[np.argmax(res)], prediction_texts[np.argmax(y_predict[i])]))\n",
    "    b.iat[i, 0] = prediction_texts[np.argmax(res)]\n",
    "    b.iat[i, 1] = prediction_texts[np.argmax(y_predict[i])]\n",
    "                                   \n",
    "b.to_csv('c:\\\\temp\\\\yone.txt')\n",
    "\n",
    "\n",
    "## 結果のプロット\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Trainig acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Trainig loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# メモ部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predict real\n",
      "1      NaN  NaN\n",
      "2      NaN  NaN\n",
      "3      NaN  NaN\n",
      "4      NaN  NaN\n",
      "5      NaN  NaN\n",
      "6      NaN  NaN\n",
      "7      NaN  NaN\n",
      "8      NaN  NaN\n",
      "9      NaN  NaN\n",
      "10     NaN  NaN\n",
      "   predict real\n",
      "1      NaN  NaN\n",
      "2        1  NaN\n",
      "3      NaN  NaN\n",
      "4      NaN  NaN\n",
      "5      NaN  NaN\n",
      "6      NaN  NaN\n",
      "7      NaN  NaN\n",
      "8      NaN  NaN\n",
      "9      NaN  NaN\n",
      "10     NaN  NaN\n",
      "hoge\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "idx = range(1, 11)\n",
    "col = [\"predict\", \"real\"]\n",
    "a = pd.DataFrame(index = idx, columns = col)\n",
    "\n",
    "print(a)\n",
    "a.iat[1, 0] = 1\n",
    "print(a)\n",
    "\n",
    "print('hoge')\n",
    "if \"\\u3000\" in \"abc\\u3000def\":\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"false\")\n",
    "#a.to_csv('c:\\\\temp\\\\yone.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Thanks\n",
    "\n",
    "## 参考リンク\n",
    "\n",
    "### \"LSTMを使ってテキストの多クラス分類する\".CODING ECHO\n",
    "https://goo.gl/MGJHTB\n",
    "\n",
    "### \"Jupyter Notebook の Tips をまとめてみた\".Tech Blog\n",
    "https://adtech.cyberagent.io/techblog/archives/2317\n",
    "\n",
    "### \"Jupyter 知っておくと少し便利なTIPS集\".Qiita\n",
    "https://qiita.com/simonritchie/items/d7dccb798f0b9c8b1ec5\n",
    "\n",
    "### \"自然言語処理における前処理の種類とその威力 \".Qiita \n",
    "livedoorニュースの加工方法に関して解説してくれている  \n",
    "https://qiita.com/Hironsan/items/2466fe0f344115aff177\n",
    "\n",
    "### \"ダウンロード - livedoor ニュースコーパス\".株式会社ロンウイット \n",
    "一定加工を行ってくれているlivedoorニュースデータを無料提供してくれている  \n",
    "https://www.rondhuit.com/download.html#ldcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python文法及びpandasなどのライブラリ使い方メモ\n",
    "\n",
    "### pandas\n",
    "\"Pandas のデータフレームを確認する\".Python でデータサイエンス   \n",
    "データフレームの行だけ表示する実装など  \n",
    "https://pythondatascience.plavox.info/pandas/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%82%92%E7%A2%BA%E8%AA%8D\n",
    "\n",
    "\"Pandas のデータフレームに行や列 (カラム) を追加する\".Python でデータサイエンス  \n",
    "https://pythondatascience.plavox.info/pandas/pandas%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%81%AB%E8%A1%8C%E3%82%84%E5%88%97%E3%82%92%E8%BF%BD%E5%8A%A0%E3%81%99%E3%82%8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KerasなどのAI関連のライブラリ使い方メモ\n",
    "### SudachiPy\n",
    "https://github.com/WorksApplications/SudachiPy  \n",
    "・githubからpipとかでインストールする必要あり\n",
    "・別途、辞書を用意する必要あり(公式の辞書も用意されている)\n",
    "\n",
    "### \"Alpine Linux上でSudachiPyを動かして形態素解析した\".Qiita\n",
    "https://qiita.com/ricesho/items/35b2e82ee3cd642cbc6a#%E6%89%8B%E9%A0%86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterNoteBookの使い方忘れそうな部分のメモ\n",
    "\n",
    "### Markdownモード\n",
    "セルを選択した状態で m を入力すれば対応可能\n",
    "その後、記載した後、Shift + Enterでラベルに変換  \n",
    "編集したい場合は、1～6で戻るのが筋がよさげ  \n",
    "rやyでも編集モードに入れるが、Markdownからコード編集モードに戻るので、数値で編集モードに入った方がよさげ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
