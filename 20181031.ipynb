{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# クリーニング部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## クリーニング処理\n",
    "def cleaning_reviews(reviews):\n",
    "    dust_keywords = ['<br />', ' ', '　', '【', '】', '(', ')','（',  '）','「', '」', '＋', '+','★', '☆', '※','~', '／', '〜', '*', '^', 'm(_ _)m', '(^^)', '(^^)', 'm(._.)m', 'ヽ=ﾟДﾟ=ﾉ']\n",
    "    for key in dust_keywords:\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{6}\\n', '', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\+[0-9]{4}\\n', '', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('http(s*)://.+\\n', '。', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('iPhone', '携帯電話')\n",
    "        reviews['NEWS'] = reviews['NEWS'].replace('\\n', '。', regex=True)\n",
    "        reviews['NEWS'] = reviews['NEWS'].str.replace(key, '')\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "##### 当メイン関数は都度実行しなくてよい(クリーニング処理をし直したい場合のみ実行)\n",
    "\n",
    "#### 前提条件\n",
    "#・livedoorニュースコーパスをダウンロードして、解凍する。\n",
    "#・textディレクトリ配下のニュースを、当JupyterNoteBookと同じ配下に配置する\n",
    "#(∵GitHubにそのままコミットして公開の位置づけにならないための配慮)\n",
    "\n",
    "## メイン関数\n",
    "import glob\n",
    "import codecs\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "count = 0\n",
    "news_index = 1\n",
    "\n",
    "df = pd.DataFrame([], columns=['CATEGORY', 'NEWS'])\n",
    "count = count + 1\n",
    "dlist1 = glob.glob(\"./text/*\")\n",
    "for dpath1 in dlist1:\n",
    "    dlist2 = glob.glob(dpath1 + \"/*\")\n",
    "    for dpath2 in dlist2:\n",
    "        if 'LICENSE.txt' not in dpath2:\n",
    "            allLines = codecs.open(dpath2, 'r', 'utf-8').read()\n",
    "            df = df.append(pd.DataFrame([[news_index, allLines]], columns=['CATEGORY', 'NEWS'], index=[count]))\n",
    "            count = count + 1\n",
    "    news_index = news_index + 1\n",
    "\n",
    "## ファイルの読込み(全件)\n",
    "#shops = pd.read_csv('shops.txt', delimiter='\\t')\n",
    "#reviews = pd.read_csv('reviews.txt', delimiter='\\t')\n",
    "\n",
    "## クリーニング\n",
    "df2 = cleaning_reviews(df)\n",
    "df2.to_csv('newsmerge.txt')\n",
    "\n",
    "## 確認用\n",
    "#print(df2.NEWS)\n",
    "#print(df2.loc[:, ['NEWS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sudachiの初期化\n",
    "import json\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import config\n",
    "\n",
    "def init_sudachi():\n",
    "    with open(config.SETTINGFILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        settings = json.load(f)\n",
    "    sudachi = dictionary.Dictionary(settings).create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C #モードC(一番長い形)に設定\n",
    "    return sudachi, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 単語の分割\n",
    "import json\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from sudachipy import config\n",
    "\n",
    "def words_tokenize(comment, sudachi, mode):\n",
    "    # tokenizeで変換した結果はオブジェクトなので、surfaceメソッドで形態素解析した結果を返却\n",
    "    return [m.surface() for m in sudachi.tokenize(mode, comment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## ラベルの作成\n",
    "def makeLabel(points):\n",
    "    result = pd.DataFrame(points)\n",
    "    for i, point in enumerate(points):\n",
    "        if point == 1:\n",
    "            result.at[i, 'CATEGORY'] = 1\n",
    "        elif point == 2:\n",
    "            result.at[i, 'CATEGORY'] = 2\n",
    "        elif point == 3:\n",
    "            result.at[i, 'CATEGORY'] = 3\n",
    "        elif point == 4:\n",
    "            result.at[i, 'CATEGORY'] = 4\n",
    "        elif point == 5:\n",
    "            result.at[i, 'CATEGORY'] = 5\n",
    "        elif point == 6:\n",
    "            result.at[i, 'CATEGORY'] = 6\n",
    "        elif point == 7:\n",
    "            result.at[i, 'CATEGORY'] = 7\n",
    "        elif point == 8:\n",
    "            result.at[i, 'CATEGORY'] = 8\n",
    "        else:\n",
    "            result.at[i, 'CATEGORY'] = 9\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## メイン関数\n",
    "import sys\n",
    "import codecs\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# ファイルの読込み(全件)\n",
    "#reviews = pd.read_csv('newsmerge.txt', delimiter=',')\n",
    "\n",
    "# データ読込み\n",
    "reviews = df\n",
    "\n",
    "## Sudachi初期化\n",
    "sudachi, mode = init_sudachi()\n",
    "\n",
    "## 単語分割\n",
    "tokenized_text_list = [words_tokenize(texts, sudachi, mode) for texts in reviews['NEWS']]\n",
    "\n",
    "## 形態素解析した結果をシーケンス変換するクラスを生成(See. https://keras.io/ja/preprocessing/text/#tokenizer)\n",
    "max_words = 100000 #一旦全部\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "## シーケンス変換するための事前学習\n",
    "tokenizer.fit_on_texts(tokenized_text_list)\n",
    "\n",
    "## シーケンス変換\n",
    "seq = tokenizer.texts_to_sequences(tokenized_text_list)\n",
    "\n",
    "## 入力値の整形(コメント側)\n",
    "maxlen = 100 #100を超える場合は切り捨て、1000を満たない場合はゼロパティング\n",
    "data = pad_sequences(seq, maxlen=maxlen)\n",
    "\n",
    "## 入力値の整形(点数側)\n",
    "labels = makeLabel(reviews['CATEGORY'])\n",
    "\n",
    "categorical_labels = to_categorical(labels)\n",
    "labels = np.asarray(categorical_labels)\n",
    "\n",
    "print(\"Shape of data tensor:{}\", format(data.shape)) # N件, maxlen\n",
    "print(\"Shape of label tensor:{}\", format(labels.shape)) # N件, 点数のカテゴリ化数\n",
    "\n",
    "# 行列をランダムにシャッフル\n",
    "#indices = np.arange(data.shape[0])\n",
    "#np.random.shuffle(indices)\n",
    "#data = data[indices]\n",
    "#labels = labels[indices]\n",
    "\n",
    "## 評価データと検算データに分割\n",
    "training_samples = data.shape[0] // 100 * 70 # \"//\" で商だけ取得可能\n",
    "validation_samples = data.shape[0] // 100 * 90 # \"//\" で商だけ取得可能\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:validation_samples]\n",
    "y_val = labels[training_samples:validation_samples]\n",
    "x_predict = data[validation_samples:]\n",
    "y_predict = labels[validation_samples:]\n",
    "\n",
    "print(\"Shape of label tensor:{}\", format(x_train.shape)) # N件, 点数のカテゴリ化数\n",
    "print(\"Shape of label tensor:{}\", format(x_val.shape)) # N件, 点数のカテゴリ化数\n",
    "print(\"Shape of label tensor:{}\", format(x_predict.shape)) # N件, 点数のカテゴリ化数\n",
    "\n",
    "## 学習モデルの作成\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 8, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(labels.shape[1], activation='sigmoid')) #点数のカテゴリ化数\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "## 学習開始\n",
    "history = model.fit(x_train, y_train, epochs=30, batch_size=8, validation_split=0.2, validation_data=(x_val, y_val))\n",
    "\n",
    "## モデルの保存\n",
    "# model.sava('pre_trained_model.h5')\n",
    "\n",
    "## 予測\n",
    "results = model.predict(x_predict)\n",
    "prediction_texts = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "\n",
    "idx = range(1, x_predict.shape[0] + 1)\n",
    "col = [\"predict\", \"real\"]\n",
    "b = pd.DataFrame(index = idx, columns = col)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "#    print('予測：{0}　答え：{1}'.format(prediction_texts[np.argmax(res)], prediction_texts[np.argmax(y_predict[i])]))\n",
    "    b.iat[i, 0] = prediction_texts[np.argmax(res)]\n",
    "    b.iat[i, 1] = prediction_texts[np.argmax(y_predict[i])]\n",
    "                                   \n",
    "b.to_csv('c:\\\\temp\\\\yone.txt')\n",
    "\n",
    "\n",
    "## 結果のプロット\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Trainig acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Trainig loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# メモ部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   predict real\n",
      "1      NaN  NaN\n",
      "2      NaN  NaN\n",
      "3      NaN  NaN\n",
      "4      NaN  NaN\n",
      "5      NaN  NaN\n",
      "6      NaN  NaN\n",
      "7      NaN  NaN\n",
      "8      NaN  NaN\n",
      "9      NaN  NaN\n",
      "10     NaN  NaN\n",
      "   predict real\n",
      "1      NaN  NaN\n",
      "2        1  NaN\n",
      "3      NaN  NaN\n",
      "4      NaN  NaN\n",
      "5      NaN  NaN\n",
      "6      NaN  NaN\n",
      "7      NaN  NaN\n",
      "8      NaN  NaN\n",
      "9      NaN  NaN\n",
      "10     NaN  NaN\n",
      "hoge\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "idx = range(1, 11)\n",
    "col = [\"predict\", \"real\"]\n",
    "a = pd.DataFrame(index = idx, columns = col)\n",
    "\n",
    "print(a)\n",
    "a.iat[1, 0] = 1\n",
    "print(a)\n",
    "\n",
    "print('hoge')\n",
    "if \"\\u3000\" in \"abc\\u3000def\":\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"false\")\n",
    "#a.to_csv('c:\\\\temp\\\\yone.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Thanks\n",
    "\n",
    "## 参考リンク\n",
    "\n",
    "### \"LSTMを使ってテキストの多クラス分類する\".CODING ECHO\n",
    "https://goo.gl/MGJHTB\n",
    "\n",
    "### \"Jupyter Notebook の Tips をまとめてみた\".Tech Blog\n",
    "https://adtech.cyberagent.io/techblog/archives/2317\n",
    "\n",
    "### \"Jupyter 知っておくと少し便利なTIPS集\".Qiita\n",
    "https://qiita.com/simonritchie/items/d7dccb798f0b9c8b1ec5\n",
    "\n",
    "### \"自然言語処理における前処理の種類とその威力 \".Qiita \n",
    "livedoorニュースの加工方法に関して解説してくれている  \n",
    "https://qiita.com/Hironsan/items/2466fe0f344115aff177\n",
    "\n",
    "### \"ダウンロード - livedoor ニュースコーパス\".株式会社ロンウイット \n",
    "一定加工を行ってくれているlivedoorニュースデータを無料提供してくれている  \n",
    "https://www.rondhuit.com/download.html#ldcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python文法及びpandasなどのライブラリ使い方メモ\n",
    "\n",
    "### pandas\n",
    "\"Pandas のデータフレームを確認する\".Python でデータサイエンス   \n",
    "データフレームの行だけ表示する実装など  \n",
    "https://pythondatascience.plavox.info/pandas/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%82%92%E7%A2%BA%E8%AA%8D\n",
    "\n",
    "\"Pandas のデータフレームに行や列 (カラム) を追加する\".Python でデータサイエンス  \n",
    "https://pythondatascience.plavox.info/pandas/pandas%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%81%AB%E8%A1%8C%E3%82%84%E5%88%97%E3%82%92%E8%BF%BD%E5%8A%A0%E3%81%99%E3%82%8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KerasなどのAI関連のライブラリ使い方メモ\n",
    "### あああ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JupyterNoteBookの使い方忘れそうな部分のメモ\n",
    "\n",
    "### Markdownモード\n",
    "セルを選択した状態で m を入力すれば対応可能\n",
    "その後、記載した後、Shift + Enterでラベルに変換  \n",
    "編集したい場合は、1～6で戻るのが筋がよさげ  \n",
    "rやyでも編集モードに入れるが、Markdownからコード編集モードに戻るので、数値で編集モードに入った方がよさげ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
